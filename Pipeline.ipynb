{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde4b498-7ebc-4ece-92a0-872b48eedf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://99ecf27bfd7b:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Recipes ML Model - Are you a dessert?</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbc01a98550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.param.shared import (\n",
    "HasInputCol, \n",
    "HasOutputCol, \n",
    "HasInputCols, \n",
    "HasOutputCols\n",
    ")\n",
    "\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml import Model\n",
    "from pyspark.ml import Estimator\n",
    "\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark=(\n",
    "    SparkSession.builder.\n",
    "    appName(\"Recipes ML Model - Are you a dessert?\").\n",
    "    config(\"spark.driver.memory\",\"8g\").\n",
    "    getOrCreate()\n",
    ")\n",
    "\n",
    "class ScalarNAFiller(\n",
    "    Transformer, \n",
    "    HasInputCol, \n",
    "    HasOutputCol, \n",
    "    HasInputCols, \n",
    "    HasOutputCols,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "\n",
    "    #estas madres existen!\n",
    "    filler = Param(\n",
    "        parent=Params._dummy(),\n",
    "        name=\"filler\",\n",
    "        doc=\"Value we want to replace our null values with.\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    @keyword_only    \n",
    "    def __init__(\n",
    "        self, \n",
    "        inputCol=None, \n",
    "        outputCol=None,\n",
    "        inputCols=None,\n",
    "        outputCols=None,\n",
    "        filler=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(filler=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    #esta tiene que estar definida!\n",
    "    #esta cosa regresa this\n",
    "    @keyword_only\n",
    "    def setParams(\n",
    "        self, \n",
    "        inputCol=None, \n",
    "        outputCol=None,\n",
    "        inputCols=None,\n",
    "        outputCols=None,\n",
    "        filler=None\n",
    "    ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setFiller(self, new_filler):\n",
    "        return self.setParams(filler=new_filler)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "\n",
    "    def setInputCols(self, new_inputCols):\n",
    "        return self.setParams(inputCols=new_inputCols)\n",
    "\n",
    "    def setOutputCols(self, new_outputCols):\n",
    "        return self.setParams(outputCols=new_outputCols)\n",
    "\n",
    "    def getFiller(self):\n",
    "        return self.getOrDefault(self.filler)\n",
    "\n",
    "    def checkParams(self):\n",
    "        if self.isSet(\"inputCol\") and self.isSet(\"inputCols\"):\n",
    "            raise ValueError(\"Only one of InputCol or InputCols must be set.\")\n",
    "\n",
    "        if not self.isSet(\"inputCol\") and not self.isSet(\"inputCols\"):\n",
    "            raise ValueError(\"One of InputCol or InputCols must be set.\")\n",
    "        \n",
    "        if self.isSet(\"inputCols\"):\n",
    "            if len(self.getInputCols()) != len(self.getOutputCols()):\n",
    "                raise ValueError(\n",
    "                    \"The length of InputCols does not match the length of outputCols.\"\n",
    "                )\n",
    "            \n",
    "        \n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        self.checkParams()\n",
    "        \n",
    "\n",
    "        input_columns = (\n",
    "            [self.getInputCol()] \n",
    "            if self.isSet(\"inputCol\") \n",
    "            else self.getInputCols()\n",
    "        )\n",
    "        \n",
    "        output_columns = (\n",
    "            [self.getOutputCol()] \n",
    "            if self.isSet(\"outputCol\") \n",
    "            else self.getOutputCols()\n",
    "        )\n",
    "\n",
    "        answer = dataset\n",
    "\n",
    "        if input_columns != output_columns:\n",
    "            for i,o in zip(input_columns,output_columns):\n",
    "                answer = answer.withColumn(o, F.col(i))\n",
    "\n",
    "        print(answer is dataset)\n",
    "        \n",
    "        na_filler = self.getFiller()\n",
    "        return answer.fillna(na_filler,subset=output_columns)\n",
    "\n",
    "\n",
    "class _ExtremeValueCapperParams(\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "\n",
    "    boundary = Param(\n",
    "        parent=Params._dummy(),\n",
    "        name=\"boundary\",\n",
    "        doc=\"Multiple of standard deviation for the cap and floor. Default = 0.0\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        #print(type(super()))\n",
    "        super().__init__(*args)\n",
    "        self._setDefault(boundary=0.0)\n",
    "\n",
    "    #se supone que no puedes hacer setboundary porque es el modelo\n",
    "    #solo en el estimador puedes hacer setboundary\n",
    "    def getBoundary(self):\n",
    "        return self.getOrDefault(self.boundary)\n",
    "\n",
    "class ExtremeValueCapperModel(Model, _ExtremeValueCapperParams):\n",
    "\n",
    "    cap = Param(\n",
    "        Params._dummy(),\n",
    "        \"cap\",\n",
    "        \"Upper bound of the values inputCol can take.\"\n",
    "        \" Values Will be capped to this value.\",\n",
    "        typeConverter=TypeConverters.toFloat\n",
    "    )\n",
    "\n",
    "    floor = Param(\n",
    "        Params._dummy(),\n",
    "        \"floor\",\n",
    "        \"Lower bound of the values inputCol can take.\"\n",
    "        \" Values Will be floored to this value.\",\n",
    "        typeConverter=TypeConverters.toFloat\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, cap=None, floor=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, cap=None, floor=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def setCap(self, new_cap):\n",
    "        return self.setParams(cap=new_cap)\n",
    "\n",
    "    def setFloor(self, new_floor):\n",
    "        return self.setParams(floor=new_floor)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "\n",
    "    #el get cap esta en el mixin\n",
    "    \n",
    "    def getCap(self):\n",
    "        return self.getOrDefault(self.cap)\n",
    "\n",
    "    def getFloor(self):\n",
    "        return self.getOrDefault(self.floor)\n",
    "\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\"No input column set for the ExtremeValueCapper model.\")\n",
    "\n",
    "        #print(self.getOutputCol())\n",
    "\n",
    "        #antes son puros strings\n",
    "        #aqui se crea la columna\n",
    "        input_column = dataset[self.getInputCol()]\n",
    "        output_column = self.getOutputCol()    \n",
    "\n",
    "        cap_value = self.getOrDefault(\"cap\")\n",
    "        floor_value = self.getOrDefault(\"floor\")\n",
    "\n",
    "        return dataset.withColumn(\n",
    "            output_column, \n",
    "            F.when(input_column > cap_value, cap_value).\n",
    "            when(input_column < floor_value, floor_value).\n",
    "            otherwise(input_column)\n",
    "        )\n",
    "\n",
    "class ExtremeValueCapper(Estimator, _ExtremeValueCapperParams):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, boundary=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, boundary=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    #se hacen todos los sets y los gets the parametros de nosotros\n",
    "\n",
    "    def setBoundary(self, new_boundary):\n",
    "        self.setParams(boundary=new_boundary)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "        \n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        input_column = self.getInputCol()\n",
    "        output_column = self.getOutputCol()\n",
    "        boundary = self.getBoundary()\n",
    "        \n",
    "        avg, stddev = dataset.agg(\n",
    "            F.mean(input_column), \n",
    "            F.stddev(input_column)\n",
    "        ).head()\n",
    "    \n",
    "        cap = avg + boundary * stddev\n",
    "        floor = avg - boundary * stddev\n",
    "    \n",
    "        return ExtremeValueCapperModel(\n",
    "            inputCol=input_column, \n",
    "            outputCol=output_column, \n",
    "            cap=cap, \n",
    "            floor=floor\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b02cf9-3874-4793-b330-3a7095f9fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.feature as MF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from typing import Optional\n",
    "\n",
    "food=spark.read.csv(\"epi_r.csv\",inferSchema=True, header=True)\n",
    "\n",
    "def sanitize_coloumn_name(name):\n",
    "\n",
    "    answer = name\n",
    "    \n",
    "    for i,j in (\n",
    "        (\" \",\"_\",),\n",
    "        (\"-\",\"_\",),\n",
    "        (\"/\",\"_\",),\n",
    "        (\"&\",\"and\",),\n",
    "    ):\n",
    "        answer = answer.replace(i,j)\n",
    "\n",
    "    return \"\".join(\n",
    "        [char for char in answer if char.isalpha() or char.isdigit() or char == \"_\"]\n",
    "    )\n",
    "\n",
    "food=food.toDF(*[sanitize_coloumn_name(name) for name in food.columns])\n",
    "\n",
    "\n",
    "food = (\n",
    "    food.where(\n",
    "        (F.col(\"cakeweek\").isin([0.0,1.0]) | F.col(\"cakeweek\").isNull()) &\n",
    "        (F.col(\"wasteless\").isin([0.0,1.0]) | F.col(\"wasteless\").isNull())\n",
    "    )\n",
    ")\n",
    "\n",
    "IDENTIFIERS = [\"title\"]\n",
    "\n",
    "TARGET = [\"dessert\"]\n",
    "\n",
    "CONTINUOUS = [\"rating\",\"calories\",\"protein\",\"fat\",\"sodium\"]\n",
    "\n",
    "BINARY = [x for x in food.columns if x not in IDENTIFIERS and x not in TARGET and x not in CONTINUOUS]\n",
    "\n",
    "print(len(BINARY))\n",
    "\n",
    "food = food.dropna(how=\"all\",subset=[x for x in food.columns if x not in IDENTIFIERS])\n",
    "food = food.dropna(subset=TARGET)\n",
    "\n",
    "@F.udf(T.BooleanType())\n",
    "def is_a_number(value: Optional[str]) -> bool:\n",
    "\n",
    "    if not value:\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        _ = float(value)\n",
    "        \n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "for column in [\"rating\", \"calories\"]:\n",
    "    food = food.where(is_a_number(F.col(column)))\n",
    "    #la va a reemplazar porq ese nombre ya existe!\n",
    "    food = food.withColumn(column, F.col(column).cast(T.DoubleType()))\n",
    "\n",
    "#ERASE TOO RARE FEATURES BINARY\n",
    "\n",
    "#TODO:CHECK IF THIS WORKS WITHOUT FILLING NA BINARY\n",
    "#food = food.fillna(value=0.0, subset=BINARY)\n",
    "\n",
    "#esto sigue funcionando sin el filla\n",
    "\n",
    "a=[F.sum(F.col(x)).alias(x) for x in BINARY]\n",
    "b=food.select(*a).head().asDict()\n",
    "num_rows = food.count()\n",
    "too_rare_features = [k for k,v in b.items() if v < 10 or v > (num_rows - 10)]\n",
    "BINARY = list(set(BINARY) - set(too_rare_features))\n",
    "\n",
    "print(len(BINARY))\n",
    "\n",
    "#RATIOS\n",
    "\n",
    "food = food.withColumn(\"protein_ratio\",F.col(\"protein\") * 4 / F.col(\"calories\"))\n",
    "food = food.withColumn(\"fat_ratio\",F.col(\"fat\") * 9 / F.col(\"calories\"))\n",
    "food = food.fillna(0.0,subset=[\"protein_ratio\",\"fat_ratio\"])\n",
    "CONTINUOUS += [\"protein_ratio\",\"fat_ratio\"]\n",
    "\n",
    "\n",
    "#CONTINUOUS\n",
    "\n",
    "scalarBIN=ScalarNAFiller(inputCols=BINARY, outputCols=BINARY, filler=0.0)\n",
    "\n",
    "evc_cal=ExtremeValueCapper(\n",
    "    inputCol=\"calories\",outputCol=\"calories\", boundary=2.0\n",
    ")\n",
    "\n",
    "evc_pro=ExtremeValueCapper(\n",
    "    inputCol=\"protein\",outputCol=\"protein\", boundary=2.0\n",
    ")\n",
    "\n",
    "evc_fat=ExtremeValueCapper(\n",
    "    inputCol=\"fat\",outputCol=\"fat\", boundary=2.0\n",
    ")\n",
    "\n",
    "evc_sod=ExtremeValueCapper(\n",
    "    inputCol=\"sodium\",outputCol=\"sodium\", boundary=2.0\n",
    ")\n",
    "\n",
    "imputer = MF.Imputer(\n",
    "    strategy=\"mean\",\n",
    "    inputCols=[\"calories\",\"protein\",\"fat\",\"sodium\"],\n",
    "    outputCols=[\"calories_i\",\"protein_i\",\"fat_i\",\"sodium_i\"]\n",
    ")\n",
    "\n",
    "continuous_assembler = MF.VectorAssembler(\n",
    "    inputCols=[\"rating\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"], \n",
    "    outputCol=\"continuous\"\n",
    ")\n",
    "\n",
    "continuous_scaler = MF.MinMaxScaler(\n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\"\n",
    ")\n",
    "\n",
    "#whole assembler\n",
    "\n",
    "preml_assembler=MF.VectorAssembler(\n",
    "    inputCols= BINARY + [\"continuous_scaled\"] + [\"protein_ratio\",\"fat_ratio\"],\n",
    "    outputCol= \"features\"\n",
    ")\n",
    "\n",
    "lr=LogisticRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"dessert\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "food_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        scalarBIN, #este es igual\n",
    "        \n",
    "        evc_cal,\n",
    "        evc_fat,\n",
    "        evc_pro,\n",
    "        evc_sod,\n",
    "        \n",
    "        imputer,\n",
    "        \n",
    "        continuous_assembler,\n",
    "        \n",
    "        continuous_scaler,\n",
    "        \n",
    "        preml_assembler,\n",
    "        \n",
    "        lr\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b66055c-68d7-4f5e-9de7-a3aa0879dd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_435a9ac45deb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c725b977-a59c-4314-821a-29d972e29fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train,test=food.randomSplit([0.7,0.3],13)\n",
    "\n",
    "#se cachea en memory!\n",
    "train.cache()\n",
    "\n",
    "#son minimos cuadrados con el probabilidad al final softmax!\n",
    "food_pipeline_model=food_pipeline.fit(train)\n",
    "results=food_pipeline_model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed041c96-6030-4540-a00e-e90d28d7c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "results\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    " labelCol=\"dessert\",\n",
    " rawPredictionCol=\"rawPrediction\",\n",
    " metricName=\"areaUnderROC\",\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc55186-9e6d-4c18-8a3f-759995f70097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.9904326300584526 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Area under ROC = {accuracy} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548b841c-4c48-4131-8f74-228c7ad2f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_pipeline_model.save(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fbc358-5cab-4891-ba68-8f952e87c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "ale=PipelineModel.read().load(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc29d53-2367-4117-a1fa-8fc53a5cf8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScalarNAFiller_874f52567f8b,\n",
       " ExtremeValueCapperModel_5413e5c3d191,\n",
       " ExtremeValueCapperModel_97b15fb36990,\n",
       " ExtremeValueCapperModel_7bffc99319bc,\n",
       " ExtremeValueCapperModel_d2f14567598d,\n",
       " ImputerModel: uid=Imputer_f6c0dffb3820, strategy=mean, missingValue=NaN, numInputCols=4, numOutputCols=4,\n",
       " VectorAssembler_b93a0c0f2d30,\n",
       " MinMaxScalerModel: uid=MinMaxScaler_edde5f5690bc, numFeatures=5, min=0.0, max=1.0,\n",
       " VectorAssembler_cb2ba25b7574,\n",
       " LogisticRegressionModel: uid=LogisticRegression_de89d5965c91, numClasses=2, numFeatures=513]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ale.stages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
