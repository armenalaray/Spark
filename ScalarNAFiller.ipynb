{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd75a85-de81-4748-aa12-968fda9b5d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://99ecf27bfd7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Recipes ML Model - Are you a dessert?</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f312ed7e910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(\n",
    "    SparkSession.builder.\n",
    "    appName(\"Recipes ML Model - Are you a dessert?\").\n",
    "    config(\"spark.driver.memory\",\"8g\").\n",
    "    getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6198bb4e-a978-4df3-b110-88250d1c7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----+\n",
      "|one|two|three|four|five|\n",
      "+---+---+-----+----+----+\n",
      "|  1|  2|    4|   1|   4|\n",
      "|  3|  6|    5|   4|   5|\n",
      "|  9|  4| NULL|   9| -99|\n",
      "| 11| 17| NULL|   3| -99|\n",
      "+---+---+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Transformer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame, Column\n",
    "\n",
    "def scalarNAFillerFunction(\n",
    "    df: DataFrame,\n",
    "    inputCol: Column,\n",
    "    outputCol: str,\n",
    "    filler: float = 0.0\n",
    "):\n",
    "    return df.withColumn(outputCol, inputCol).fillna(filler,subset=outputCol)   \n",
    "    \n",
    "\n",
    "df=spark.createDataFrame(\n",
    "    data=[[1, 2, 4, 1], [3, 6, 5, 4], [9, 4, None, 9], [11, 17, None, 3]],\n",
    "    schema=[\"one\",\"two\",\"three\",\"four\"]\n",
    ")\n",
    "\n",
    "scalarNAFillerFunction(df, F.col(\"three\"), \"five\", -99.0).show()\n",
    "\n",
    "#df.withColumn(\"five\",col=F.col(\"three\")).fillna(-99.0,subset=\"five\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28668aff-4f6e-407e-9b9d-e2b0f54d50a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='undefined', name='filler', doc='Value we want to replace our null values with.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "\n",
    "filler = Param(\n",
    "    parent=Params._dummy(),\n",
    "    name=\"filler\",\n",
    "    doc=\"Value we want to replace our null values with.\",\n",
    "    typeConverter=TypeConverters.toFloat,\n",
    ")\n",
    "\n",
    "filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93694482-ae28-415e-b67f-ee9ca4f3d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+\n",
      "|one|two|three|four|\n",
      "+---+---+-----+----+\n",
      "|  1|  2|    4|   1|\n",
      "|  3|  6|    5|   4|\n",
      "|  9|  4| NULL|   9|\n",
      "| 11| 17| NULL|   3|\n",
      "+---+---+-----+----+\n",
      "\n",
      "+---+---+-----+----+-----+\n",
      "|one|two|three|four| five|\n",
      "+---+---+-----+----+-----+\n",
      "|  1|  2|    4|   1|  4.0|\n",
      "|  3|  6|    5|   4|  5.0|\n",
      "|  9|  4| NULL|   9|-99.0|\n",
      "| 11| 17| NULL|   3|-99.0|\n",
      "+---+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark import keyword_only\n",
    "\n",
    "class ScalarNAFiller(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    #estas madres existen!\n",
    "    filler = Param(\n",
    "        parent=Params._dummy(),\n",
    "        name=\"filler\",\n",
    "        doc=\"Value we want to replace our null values with.\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    @keyword_only    \n",
    "    def __init__(self, inputCol=None, outputCol=None, filler=None):\n",
    "        super().__init__()\n",
    "        #print(self._setDefault)\n",
    "        self._setDefault(filler=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    #esta tiene que estar definida!\n",
    "    #esta cosa regresa this\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, filler=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setFiller(self, new_filler):\n",
    "        return self.setParams(filler=new_filler)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "\n",
    "    def getFiller(self):\n",
    "        return self.getOrDefault(self.filler)\n",
    "    \n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\"No input column set for the ScalarNAFiller transformer.\")\n",
    "\n",
    "        input_column = dataset[self.getInputCol()]\n",
    "        output_column = self.getOutputCol()\n",
    "\n",
    "        na_filler = self.getFiller()\n",
    "\n",
    "        return dataset.withColumn(\n",
    "            output_column, \n",
    "            input_column.cast(\"double\")\n",
    "        ).fillna(\n",
    "            na_filler,\n",
    "            subset=output_column\n",
    "        )\n",
    "\n",
    "a=ScalarNAFiller(inputCol=\"three\", outputCol=\"five\", filler=-99)\n",
    "\n",
    "df.show()\n",
    "a.transform(df).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3863609f-35e8-46e8-a90a-0379de12dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----+\n",
      "|one|two|three|four|five|\n",
      "+---+---+-----+----+----+\n",
      "|  1|  2|    4|   1| 4.0|\n",
      "|  3|  6|    5|   4| 5.0|\n",
      "|  9|  4| NULL|   9|17.0|\n",
      "| 11| 17| NULL|   3|17.0|\n",
      "+---+---+-----+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.setFiller(17).transform(df).show()\n",
    "a.getFiller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bba3492-5b95-4db4-ab31-59c946701424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----+\n",
      "|one|two|three|four|five|\n",
      "+---+---+-----+----+----+\n",
      "|  1|  2|    4|   1| 4.0|\n",
      "|  3|  6|    5|   4| 5.0|\n",
      "|  9|  4| NULL|   9|40.0|\n",
      "| 11| 17| NULL|   3|40.0|\n",
      "+---+---+-----+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transform(df, params={a.filler:40}).show()\n",
    "a.getFiller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd0fdb1-4817-4641-8dcc-995c30bfb2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='ScalarNAFiller_6c1a06ed4dc9', name='filler', doc='Value we want to replace our null values with.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbb27f-0da4-4b75-88fb-4ef7794befea",
   "metadata": {},
   "source": [
    "#ExtremeValueCapper is an estimator\n",
    "\n",
    "create the model clas as a Model which is a Transformer\n",
    "\n",
    "Any value given by the floor replaced floor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4b87f7-9cdc-4b96-8bb3-632c1e5adad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----------+\n",
      "|one|two|three|four|new_column|\n",
      "+---+---+-----+----+----------+\n",
      "|  1|  2|    4|   1|       1.0|\n",
      "|  3|  6|    5|   4|       1.0|\n",
      "|  9|  4| NULL|   9|       1.0|\n",
      "| 11| 17| NULL|   3|       1.0|\n",
      "+---+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_transform(\n",
    "    df: DataFrame,\n",
    "    inputCol: Column,\n",
    "    outputCol: str,\n",
    "    cap: float,\n",
    "    floor: float,\n",
    "):\n",
    "    return df.withColumn(\n",
    "        outputCol, \n",
    "        F.when(inputCol > cap, cap).\n",
    "        when(inputCol < floor, floor).\n",
    "        otherwise(inputCol)\n",
    "    )\n",
    "\n",
    "model_transform(df,F.col(\"four\"),\"new_column\",1.0,0.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f333bbb4-15cf-4b38-883c-10a4998ad1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.760952285695232 1.2390477143047667\n",
      "+---+---+-----+----+------------------+\n",
      "|one|two|three|four|        new_column|\n",
      "+---+---+-----+----+------------------+\n",
      "|  1|  2|    4|   1|1.2390477143047667|\n",
      "|  3|  6|    5|   4|               3.0|\n",
      "|  9|  4| NULL|   9|               9.0|\n",
      "| 11| 17| NULL|   3|10.760952285695232|\n",
      "+---+---+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def estimator_fit(\n",
    "    df: DataFrame, \n",
    "    inputCol: Column, \n",
    "    outputCol: str, \n",
    "    boundary: float\n",
    "):\n",
    "    avg, stddev = df.agg(F.mean(inputCol), F.stddev(inputCol)).head()\n",
    "    \n",
    "    cap = avg + boundary * stddev\n",
    "    floor = avg - boundary * stddev\n",
    "\n",
    "    print(cap, floor)\n",
    "    return model_transform(df, inputCol, outputCol, cap, floor)\n",
    "\n",
    "\n",
    "estimator_fit(df,F.col(\"one\"),\"new_column\",1.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10d9e8d1-8ec7-4c7b-96aa-cafc1f027221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ExtremeValueCapperParams_eb73005c60b7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class _ExtremeValueCapperParams(HasInputCol,HasOutputCol):\n",
    "\n",
    "    boundary = Param(\n",
    "        parent=Params._dummy(),\n",
    "        name=\"boundary\",\n",
    "        doc=\"Multiple of standard deviation for the cap and floor. Default = 0.0\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        #print(type(super()))\n",
    "        super().__init__(*args)\n",
    "        self._setDefault(boundary=0.0)\n",
    "\n",
    "    #se supone que no puedes hacer setboundary porque es el modelo\n",
    "    #solo en el estimador puedes hacer setboundary\n",
    "    def getBoundary(self):\n",
    "        return self.getOrDefault(self.boundary)\n",
    "    \n",
    "\n",
    "    #no tiene el transform no puede ser llamado!\n",
    "\n",
    "\n",
    "_ExtremeValueCapperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "93c9f37b-584e-4e34-b0b9-e4ce62e56aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----------+\n",
      "|one|two|three|four|new_column|\n",
      "+---+---+-----+----+----------+\n",
      "|  1|  2|    4|   1|       0.9|\n",
      "|  3|  6|    5|   4|       0.9|\n",
      "|  9|  4| NULL|   9|       0.9|\n",
      "| 11| 17| NULL|   3|       0.9|\n",
      "+---+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml import Model\n",
    "\n",
    "class ExtremeValueCapperModel(Model, _ExtremeValueCapperParams):\n",
    "\n",
    "    cap = Param(\n",
    "        Params._dummy(),\n",
    "        \"cap\",\n",
    "        \"Upper bound of the values inputCol can take.\"\n",
    "        \" Values Will be capped to this value.\",\n",
    "        typeConverter=TypeConverters.toFloat\n",
    "    )\n",
    "\n",
    "    floor = Param(\n",
    "        Params._dummy(),\n",
    "        \"floor\",\n",
    "        \"Lower bound of the values inputCol can take.\"\n",
    "        \" Values Will be floored to this value.\",\n",
    "        typeConverter=TypeConverters.toFloat\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, cap=None, floor=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, cap=None, floor=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def setCap(self, new_cap):\n",
    "        return self.setParams(cap=new_cap)\n",
    "\n",
    "    def setFloor(self, new_floor):\n",
    "        return self.setParams(floor=new_floor)\n",
    "\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "\n",
    "\n",
    "    \n",
    "    def getCap(self):\n",
    "        return self.getOrDefault(self.cap)\n",
    "\n",
    "    def getFloor(self):\n",
    "        return self.getOrDefault(self.floor)\n",
    "\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\"No input column set for the ExtremeValueCapper model.\")\n",
    "\n",
    "        #print(self.getOutputCol())\n",
    "\n",
    "        #antes son puros strings\n",
    "        #aqui se crea la columna\n",
    "        input_column = df[self.getInputCol()]\n",
    "        output_column = self.getOutputCol()    \n",
    "\n",
    "        cap_value = self.getOrDefault(\"cap\")\n",
    "        floor_value = self.getOrDefault(\"floor\")\n",
    "\n",
    "        return df.withColumn(\n",
    "            output_column, \n",
    "            F.when(input_column > cap_value, cap_value).\n",
    "            when(input_column < floor_value, floor_value).\n",
    "            otherwise(input_column)\n",
    "        )\n",
    "        \n",
    "\n",
    "ExtremeValueCapperModel(\n",
    "    inputCol=\"one\", \n",
    "    outputCol=\"new_column\", \n",
    "    cap=0.9, \n",
    "    floor=0.9).transform(df).show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
